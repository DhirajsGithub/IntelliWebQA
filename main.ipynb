{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ab51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "import langchain\n",
    "\n",
    "import os\n",
    "GEMINI_KEY = os.getenv('GEMINI_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7550dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the llm\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GEMINI_KEY,\n",
    "    temperature=0.6,\n",
    "    max_tokens=600,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d026b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading some url using langchain web base loader\n",
    "\n",
    "loader = WebBaseLoader([\n",
    "    \"https://www.moneycontrol.com/news/business/banks/hdfc-bank-re-appoints-sanmoy-chakrabarti-as-chief-risk-officer-11259771.html\",\n",
    "    \"https://www.moneycontrol.com/news/business/markets/market-corrects-post-rbi-ups-inflation-forecast-icrr-bet-on-these-top-10-rate-sensitive-stocks-ideas-11142611.html\"\n",
    "])\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf80514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into chunks using langchain recursive text splitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,  \n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"] \n",
    ")\n",
    "\n",
    "# chunks = splitter.split_text(data)\n",
    "chunks = splitter.split_documents(data)\n",
    "chunks_str = [chunk.page_content for chunk in chunks]\n",
    "len(chunks_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding of chunks using google genai embedding (different libraries can be used but since we are using google genai llm, better to use that embedding only)\n",
    "# make sure your os has GOOGLE_API_KEY set\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# models/embedding-001 is a pre-trained embedding model provided by Google's Gemini API.\n",
    "# It is designed to produce a 768-dimensional vector for each input string, regardless of the length of the text (though longer text may be truncated or summarized internally).\n",
    "\n",
    "vectors = embeddings.embed_documents(chunks_str)\n",
    "vectors_rows = len(vectors)\n",
    "vectors_cols = len(vectors[0])    \n",
    "print(vectors_rows)\n",
    "print(vectors_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b584cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a faiss index for vectors\n",
    "# Step 3: Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "# Step 4: Save it to disk\n",
    "vectorstore.save_local(\"my_faiss_index\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it again later using the same embedding model\n",
    "vectorstore = FAISS.load_local(\n",
    "\t\"my_faiss_index\",\n",
    "\tembeddings=embeddings,\n",
    "\tallow_dangerous_deserialization=True  # Only set to True if you trust the file source\n",
    ")\n",
    "\n",
    "# use the similarity search\n",
    "# results = vectorstore.similarity_search(\"Tesla stock\", k=2)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fd6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve similar embeddings for a given question and call LLM to retrieve final answer\n",
    "# Create a question-answering chain with source citations\n",
    "# It retrieves relevant documents from the vector store and uses the LLM to answer the question\n",
    "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672707b5",
   "metadata": {},
   "source": [
    "### Stuff Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31491d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STUFF METHOD ===\n",
    "# 1. Retrieve top-k relevant document chunks from the vector store.\n",
    "# 2. Concatenate (stuff) all retrieved chunks into a single prompt.\n",
    "# 3. Send that combined context + question to the LLM in one call.\n",
    "# 4. LLM generates the final answer in a single step.\n",
    "#\n",
    "# ✅ Simpler and faster.\n",
    "# ⚠️ May hit token limits if many or long documents are retrieved.\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "# chain \n",
    "\n",
    "# ask question\n",
    "query = \"what has been appointed for a period of five years\"\n",
    "# query = \"what are the main features of punch iCNG?\"\n",
    "# langchain.debug=True\n",
    "chain({\"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02dc989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "945cb982",
   "metadata": {},
   "source": [
    "### Map-Reduce Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MAP-REDUCE METHOD ===\n",
    "# 1. Retrieve top-k relevant document chunks from the vector store.\n",
    "# 2. MAP: Pass each chunk individually to the LLM with the same question.\n",
    "#    - The LLM returns an answer or summary per chunk (e.g., fc1, fc2, fc3...).\n",
    "# 3. REDUCE: Combine all intermediate outputs and pass them again to the LLM.\n",
    "#    - The LLM synthesizes a final, aggregated answer.\n",
    "#\n",
    "# ✅ Handles longer documents and supports more complex reasoning.\n",
    "# ⚠️ Slower and more expensive (multiple LLM calls).\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# ask question\n",
    "query = \"what has been appointed for a period of five years\"\n",
    "# query = \"what are the main features of punch iCNG?\"\n",
    "# langchain.debug=True\n",
    "result = chain(query)\n",
    "print(result[\"result\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_research_tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
