{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4712e2",
   "metadata": {},
   "source": [
    "### Text loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3034aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('nvda_news_1.txt')\n",
    "data = loader.load()\n",
    "# data[0]\n",
    "# data[0].page_content\n",
    "data[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47785e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader('movies.csv')\n",
    "data = loader.load()\n",
    "# data[0].metadata\n",
    "# data[0].page_content\n",
    "loader = CSVLoader('movies.csv', source_column='title')\n",
    "data[0].metadata\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0fe32",
   "metadata": {},
   "source": [
    "### Unstructured url loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install unstructured libmagic python-magic python-magic-bin\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader([\n",
    "    \"https://www.moneycontrol.com/news/business/banks/hdfc-bank-re-appoints-sanmoy-chakrabarti-as-chief-risk-officer-11259771.html\",\n",
    "    \"https://www.moneycontrol.com/news/business/markets/market-corrects-post-rbi-ups-inflation-forecast-icrr-bet-on-these-top-10-rate-sensitive-stocks-ideas-11142611.html\"\n",
    "])\n",
    "\n",
    "docs = loader.load()\n",
    "newsContent = docs[0].page_content\n",
    "print(newsContent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a594d396",
   "metadata": {},
   "source": [
    "### Text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ebb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator='\\n',          # Split on newline characters\n",
    "    chunk_size=200,          # Each chunk is at most 200 characters long\n",
    "    chunk_overlap=0          # No overlapping content between chunks\n",
    "    # Number of characters to overlap between consecutive chunks.\n",
    "    # This helps preserve some context from the end of one chunk into the start of the next.\n",
    "    # Set to 0 for no overlap, or a higher value (e.g., 50) to retain context across chunks.\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(newsContent)  # `docs` is your input string\n",
    "len(chunks)                         # Returns the number of resulting chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be069fc0",
   "metadata": {},
   "source": [
    "CharacterTextSplitter\n",
    "\n",
    "üîª Drawbacks of CharacterTextSplitter:\n",
    "- May split in the middle of words or sentences, leading to poor chunk quality.\n",
    "- No language or token awareness, so it can break context or underutilize model capacity.\n",
    "- Not ideal for semantic tasks like summarization or Q&A.\n",
    "\n",
    "‚úÖ Use Cases:\n",
    "- Best for simple, line-separated data (e.g., logs, code).\n",
    "- Useful when you need quick and lightweight splitting without language overhead.\n",
    "- Good for preprocessing short texts where sentence structure isn‚Äôt critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,  # Overlap helps maintain context between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]  # Tries these in order for best natural splits\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(newsContent)\n",
    "len(chunks)\n",
    "\n",
    "# Loop to inspect chunk details\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(f\"Length: {len(chunk)} characters\")\n",
    "    print(chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba50643",
   "metadata": {},
   "source": [
    "‚úÖ Use Cases of RecursiveCharacterTextSplitter:\n",
    "Text summarization\n",
    "Keeps sentences/paragraphs intact for better model understanding.\n",
    "\n",
    "Question answering over documents\n",
    "Preserves context so questions can refer to nearby sentences.\n",
    "\n",
    "Search + retrieval (RAG)\n",
    "Splits into dense, semantically complete chunks for embedding & retrieval.\n",
    "\n",
    "‚ùå Drawbacks of RecursiveCharacterTextSplitter:\n",
    "Slower than simple splitters\n",
    "Due to recursive logic and merging steps.\n",
    "\n",
    "Uneven chunk sizes\n",
    "Chunks can vary depending on where natural breaks occur.\n",
    "\n",
    "Not ideal for structured/tabular text\n",
    "Like logs or code where natural language structure isn‚Äôt relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699cf7c4",
   "metadata": {},
   "source": [
    "**Merging** is the process of recombining small fragments (after splitting) into meaningful chunks that meet a certain chunk_size while preserving natural structure (like paragraphs or sentences).\n",
    "\n",
    "‚úÖ Why it matters:\n",
    "When using splitters like RecursiveCharacterTextSplitter, it:\n",
    "Splits using natural boundaries (like \\n\\n, ., , etc.)\n",
    "Then merges smaller pieces back together into chunks up to chunk_size with optional chunk_overlap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5316b",
   "metadata": {},
   "source": [
    "### Faiss tutorial\n",
    "```\n",
    "!pip install faiss-cpu\n",
    "!pip install sentence-transformers\n",
    "```\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library made by Facebook AI Research. It's used to search for similar items (like text or images) very quickly.\n",
    "- Finding similar documents\n",
    "- Recommending items\n",
    "- Clustering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df = pd.read_csv(\"sample_text.csv\")\n",
    "df.shape\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c030f",
   "metadata": {},
   "source": [
    "### Step 1 : Create source embeddings for the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d818041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "encoder = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346bc240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculate embeddings by calling encoder.encode()\n",
    "sentences = df.text\n",
    "embeddings = encoder.encode(sentences)        # embeddings similar to vector,\n",
    "# embeddings are specifically learned representations that capture semantic meaning and relationships between data points\n",
    "print(embeddings.shape)\n",
    "print(embeddings)\n",
    "# 3. Calculate the embedding similarities\n",
    "# similarities = encoder.similarity(embeddings, embeddings)\n",
    "# print(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = embeddings.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f3a12",
   "metadata": {},
   "source": [
    "### Step 2 : Build a FAISS Index for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b8516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "# This creates an index (a data structure for searching) that uses L2 distance, which is another name for Euclidean distance.\n",
    "# ‚ÄúFlat‚Äù means it does brute-force search (it checks all vectors) ‚Äî simple but accurate.\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f3eeb",
   "metadata": {},
   "source": [
    "### Step 3 : Normalize the source vectors (as we are using L2 distance to measure similarity) and add to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dee9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(embeddings)\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040732b",
   "metadata": {},
   "source": [
    "### Step 4 : Encode search text using same encorder and normalize the output vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a natural language search query. You can change it to test other queries.\n",
    "search_query = \"I want to go to manali\"\n",
    "# search_query = \"looking for places to visit during the holidays\"\n",
    "# search_query = \"An apple a day keeps the doctor away\"\n",
    "\n",
    "vec = encoder.encode(search_query)\n",
    "# Convert the search query into a vector (embedding) using a sentence encoder.\n",
    "# The result is typically a 1D array of floats, e.g., shape (768,)\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "svec = np.array(vec).reshape(1,-1)\n",
    "# Convert the 1D vector into a 2D array with shape (1, 768)\n",
    "# This is needed because many libraries (like FAISS) expect input as a batch of vectors (n, dim)\n",
    "\n",
    "svec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a364f16",
   "metadata": {},
   "source": [
    "### Step 5: Search for similar vector in the FAISS index created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85938f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, I = index.search(svec, k=2)\n",
    "# Search the FAISS index for the top-k most similar vectors to 'svec'\n",
    "# 'k=2' means: return the top 2 closest matches (nearest neighbors)\n",
    "# Returns two things:\n",
    "#  - 'distances': the similarity (or distance) scores to the top-k vectors\n",
    "#  - 'I': the indices (positions) of the top-k vectors in the index\n",
    "\n",
    "print(distances)\n",
    "print(I)\n",
    "# Show the similarity (or distance) scores\n",
    "# If you're using IndexFlatIP ‚Üí higher is better (inner product similarity)\n",
    "# If you're using IndexFlatL2 ‚Üí lower is better (shorter Euclidean distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca159b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "I.tolist()\n",
    "row_indices = I.tolist()[0]\n",
    "row_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832af84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df.loc[row_indices]\n",
    "print(search_query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ec3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_research_tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
